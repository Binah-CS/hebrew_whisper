{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "install_libraries",
        "outputId": "0d0f7a6d-e6c5-4f6f-e8e2-8b5b5b5b5b5b"
      },
      "outputs": [],
      "source": [
        "!pip install gradio numpy torch transformers librosa soundfile tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        },
        "id": "main_code",
        "outputId": "2c2f2f2f-2f2f-2f2f-2f2f-2f2f2f2f2f2f"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "import librosa\n",
        "import os\n",
        "import tempfile\n",
        "import soundfile as sf\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "import re\n",
        "\n",
        "SAMPLING_RATE = 16000\n",
        "\n",
        "model_name = 'ivrit-ai/whisper-large-v2-tuned'\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = WhisperForConditionalGeneration.from_pretrained(model_name).to(device)\n",
        "processor = WhisperProcessor.from_pretrained(model_name)\n",
        "\n",
        "def find_silent_sections(audio_data, sr, min_silence_length=0.5, silence_threshold=-40):\n",
        "    \"\"\"\n",
        "    Find silent sections in an audio file.\n",
        "    \n",
        "    :param audio_data: Numpy array of the audio data.\n",
        "    :param sr: Sampling rate of the audio data.\n",
        "    :param min_silence_length: Minimum length of silence to detect (in seconds).\n",
        "    :param silence_threshold: The threshold (in dB) below which, the segment is considered silent.\n",
        "    :return: A list of tuples indicating the start and end samples of silent sections.\n",
        "    \"\"\"\n",
        "    # Identify silent sections\n",
        "    silent_sections = librosa.effects.split(audio_data, top_db=-silence_threshold)\n",
        "    # Filter out short silent sections\n",
        "    silent_sections = [s for s in silent_sections if (s[1] - s[0]) >= min_silence_length * sr]\n",
        "    \n",
        "    return silent_sections\n",
        "\n",
        "def split_into_paragraphs(text, min_words_per_paragraph=20):\n",
        "    \"\"\"\n",
        "    Split the text into paragraphs based on sentence endings and paragraph length.\n",
        "    \n",
        "    :param text: The input text to be split into paragraphs.\n",
        "    :param min_words_per_paragraph: The minimum number of words required in a paragraph.\n",
        "    :return: A list of paragraphs.\n",
        "    \"\"\"\n",
        "    # Split the text into sentences\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    \n",
        "    paragraphs = []\n",
        "    current_paragraph = \"\"\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        current_paragraph += sentence + \" \"\n",
        "        \n",
        "        if len(current_paragraph.split()) >= min_words_per_paragraph:\n",
        "            paragraphs.append(current_paragraph.strip())\n",
        "            current_paragraph = \"\"\n",
        "    \n",
        "    if current_paragraph:\n",
        "        paragraphs.append(current_paragraph.strip())\n",
        "    \n",
        "    return paragraphs\n",
        "\n",
        "def transcribe_and_translate(audio_file, source_language):\n",
        "    if not source_language or (isinstance(source_language, list) and not source_language[0]):\n",
        "        return \"Source language was not selected. Please choose a language.\"\n",
        "\n",
        "    if isinstance(source_language, list):\n",
        "        source_language = source_language[0]\n",
        "    \n",
        "    audio, rate = librosa.load(audio_file, sr=None)\n",
        "\n",
        "    if rate != SAMPLING_RATE:\n",
        "        audio = librosa.resample(audio, orig_sr=rate, target_sr=SAMPLING_RATE)\n",
        "    \n",
        "    temp_dir = tempfile.mkdtemp()\n",
        "    \n",
        "    chunk_duration = 30  # Duration in seconds\n",
        "    chunks = []\n",
        "\n",
        "    silent_sections = find_silent_sections(audio, SAMPLING_RATE, min_silence_length=0.3, silence_threshold=-40)\n",
        "    start_idx = 0\n",
        "    for end_idx in [s[0] for s in silent_sections]:\n",
        "        if end_idx - start_idx > SAMPLING_RATE * chunk_duration:\n",
        "            chunks.append(audio[start_idx:end_idx])\n",
        "            start_idx = end_idx\n",
        "\n",
        "    # Handling the last chunk\n",
        "    if len(audio) - start_idx > 0:\n",
        "        chunks.append(audio[start_idx:])\n",
        "\n",
        "    transcribed_text = \"\"\n",
        "    for i, chunk in enumerate(tqdm(chunks, desc=\"Processing chunks\")):\n",
        "        chunk_path = os.path.join(temp_dir, f\"chunk_{i}.wav\")\n",
        "        sf.write(chunk_path, chunk, samplerate=SAMPLING_RATE)\n",
        "        \n",
        "        chunk_audio, _ = librosa.load(chunk_path, sr=SAMPLING_RATE)\n",
        "        \n",
        "        input_features = processor(chunk_audio, sampling_rate=SAMPLING_RATE, return_tensors=\"pt\").input_features.to(device)\n",
        "        \n",
        "        predicted_ids = model.generate(input_features, language=source_language, num_beams=5)\n",
        "        chunk_text = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
        "        \n",
        "        transcribed_text += chunk_text + \" \"\n",
        "        print(f\"Processed chunk {i+1}/{len(chunks)}\")\n",
        "    \n",
        "    # Split the transcribed text into paragraphs\n",
        "    paragraphs = split_into_paragraphs(transcribed_text)\n",
        "    \n",
        "    # Delete the temporary directory\n",
        "    shutil.rmtree(temp_dir)\n",
        "    \n",
        "    return \"\\n\\n\".join(paragraphs)\n",
        "\n",
        "title = \"Unlimited Length Transcription and Translation\"\n",
        "description = \"With ivrit-ai/whisper-large-v2-tuned | GUI by Shmuel Ronen\"\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=transcribe_and_translate,\n",
        "    inputs=[\n",
        "        gr.Audio(type=\"filepath\", label=\"Upload Audio File\"),\n",
        "        gr.Dropdown(choices=['Hebrew', 'English', 'Spanish', 'French'], label=\"Source Language\")\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Transcription / Translation\"),\n",
        "    title=title,\n",
        "    description=description\n",
        ")\n",
        "\n",
        "interface.launch()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}